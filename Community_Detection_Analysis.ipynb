{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection Analysis for Knowledge Graph\n",
    "\n",
    "This notebook performs comprehensive community detection analysis on a relational knowledge graph.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Graph Construction\n",
    "2. Graph Statistics and Exploration\n",
    "3. Community Detection Algorithms\n",
    "4. Quality Metrics Evaluation\n",
    "5. Comparative Analysis\n",
    "6. Visualizations\n",
    "7. Results Export\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "NetworkX version: 3.6.1\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NetworkX version: {nx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading knowledge graph...\n",
      "✓ Loaded graph with 1316 nodes and 7480 edges\n"
     ]
    }
   ],
   "source": [
    "def load_knowledge_graph(filepath):\n",
    "    \"\"\"\n",
    "    Load knowledge graph from triplet format (subject relation object)\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the knowledge graph file\n",
    "    \n",
    "    Returns:\n",
    "        G: NetworkX graph\n",
    "        edges_with_relations: List of tuples (subject, object, relation)\n",
    "    \"\"\"\n",
    "    print(\"Loading knowledge graph...\")\n",
    "    G = nx.Graph()\n",
    "    edges_with_relations = []\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 3:\n",
    "                subject, relation, obj = parts[0], parts[1], parts[2]\n",
    "                G.add_edge(subject, obj, relation=relation)\n",
    "                edges_with_relations.append((subject, obj, relation))\n",
    "    \n",
    "    print(f\"✓ Loaded graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    return G, edges_with_relations\n",
    "\n",
    "# Load the graph\n",
    "filepath = 'train.txt'  # Update this path as needed\n",
    "G, edges = load_knowledge_graph(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Graph Statistics and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_statistics(G):\n",
    "    \"\"\"\n",
    "    Print comprehensive graph statistics\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"KNOWLEDGE GRAPH STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    print(f\"  Nodes: {G.number_of_nodes():,}\")\n",
    "    print(f\"  Edges: {G.number_of_edges():,}\")\n",
    "    print(f\"  Density: {nx.density(G):.6f}\")\n",
    "    print(f\"  Average Degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
    "    print(f\"  Average Clustering Coefficient: {nx.average_clustering(G):.4f}\")\n",
    "    \n",
    "    print(f\"\\nConnectivity:\")\n",
    "    print(f\"  Is Connected: {nx.is_connected(G)}\")\n",
    "    \n",
    "    if nx.is_connected(G):\n",
    "        print(f\"  Diameter: {nx.diameter(G)}\")\n",
    "        print(f\"  Average Shortest Path Length: {nx.average_shortest_path_length(G):.2f}\")\n",
    "    else:\n",
    "        num_components = nx.number_connected_components(G)\n",
    "        print(f\"  Number of Connected Components: {num_components}\")\n",
    "        largest_cc = max(nx.connected_components(G), key=len)\n",
    "        print(f\"  Largest Component Size: {len(largest_cc)} nodes ({len(largest_cc)/G.number_of_nodes()*100:.1f}%)\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print_graph_statistics(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relation types\n",
    "relation_counts = Counter()\n",
    "for _, _, data in G.edges(data=True):\n",
    "    relation_counts[data.get('relation', 'unknown')] += 1\n",
    "\n",
    "print(\"Top 10 Relation Types:\")\n",
    "for relation, count in relation_counts.most_common(10):\n",
    "    print(f\"  {relation}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(G, communities):\n",
    "    \"\"\"Compute modularity of community partition\"\"\"\n",
    "    return community.modularity(G, communities)\n",
    "\n",
    "def compute_coverage(G, communities):\n",
    "    \"\"\"Compute coverage - fraction of edges within communities\"\"\"\n",
    "    from networkx.algorithms.community.quality import intra_community_edges\n",
    "    total_edges = G.number_of_edges()\n",
    "    if total_edges == 0:\n",
    "        return 0.0\n",
    "    intra_edges = intra_community_edges(G, communities)\n",
    "    return intra_edges / total_edges\n",
    "\n",
    "def compute_performance(G, communities):\n",
    "    \"\"\"Compute performance metric - fraction of node pairs correctly classified\"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    if n <= 1:\n",
    "        return 1.0\n",
    "    \n",
    "    # Create community membership dict\n",
    "    node_to_comm = {}\n",
    "    for i, comm in enumerate(communities):\n",
    "        for node in comm:\n",
    "            node_to_comm[node] = i\n",
    "    \n",
    "    # For large graphs, sample pairs\n",
    "    max_pairs = 10000\n",
    "    nodes = list(G.nodes())\n",
    "    total_possible = n * (n - 1) // 2\n",
    "    \n",
    "    if total_possible <= max_pairs:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(i + 1, len(nodes)):\n",
    "                total += 1\n",
    "                u, v = nodes[i], nodes[j]\n",
    "                same_community = node_to_comm.get(u) == node_to_comm.get(v)\n",
    "                has_edge = G.has_edge(u, v)\n",
    "                if (same_community and has_edge) or (not same_community and not has_edge):\n",
    "                    correct += 1\n",
    "    else:\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        correct = 0\n",
    "        total = max_pairs\n",
    "        for _ in range(max_pairs):\n",
    "            u, v = random.sample(nodes, 2)\n",
    "            same_community = node_to_comm.get(u) == node_to_comm.get(v)\n",
    "            has_edge = G.has_edge(u, v)\n",
    "            if (same_community and has_edge) or (not same_community and not has_edge):\n",
    "                correct += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def compute_community_diameter(G, communities):\n",
    "    \"\"\"Compute diameter for each community\"\"\"\n",
    "    diameters = []\n",
    "    for comm in communities:\n",
    "        subgraph = G.subgraph(comm)\n",
    "        if nx.is_connected(subgraph) and len(comm) > 1:\n",
    "            diameters.append(nx.diameter(subgraph))\n",
    "        else:\n",
    "            diameters.append(0)\n",
    "    return diameters\n",
    "\n",
    "def compute_community_density(G, communities):\n",
    "    \"\"\"Compute density for each community\"\"\"\n",
    "    densities = []\n",
    "    for comm in communities:\n",
    "        subgraph = G.subgraph(comm)\n",
    "        densities.append(nx.density(subgraph))\n",
    "    return densities\n",
    "\n",
    "def compute_clustering_coefficient(G, communities):\n",
    "    \"\"\"Compute average clustering coefficient for each community\"\"\"\n",
    "    clustering_coeffs = []\n",
    "    for comm in communities:\n",
    "        subgraph = G.subgraph(comm)\n",
    "        if len(comm) > 1:\n",
    "            clustering_coeffs.append(nx.average_clustering(subgraph))\n",
    "        else:\n",
    "            clustering_coeffs.append(0.0)\n",
    "    return clustering_coeffs\n",
    "\n",
    "def compute_conductance(G, communities):\n",
    "    \"\"\"Compute conductance for each community\"\"\"\n",
    "    conductances = []\n",
    "    for comm in communities:\n",
    "        if len(comm) > 0:\n",
    "            internal_edges = G.subgraph(comm).number_of_edges()\n",
    "            cut_edges = nx.cuts.cut_size(G, comm)\n",
    "            vol_s = 2 * internal_edges + cut_edges\n",
    "            vol_total = 2 * G.number_of_edges()\n",
    "            vol_complement = vol_total - vol_s\n",
    "            \n",
    "            if vol_s > 0 and vol_complement > 0:\n",
    "                conductance = cut_edges / min(vol_s, vol_complement)\n",
    "            else:\n",
    "                conductance = 0\n",
    "            conductances.append(conductance)\n",
    "        else:\n",
    "            conductances.append(0)\n",
    "    return conductances\n",
    "\n",
    "print(\"✓ Quality metric functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Community Detection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Louvain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOUVAIN ALGORITHM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run Louvain\n",
    "communities_louvain = community.louvain_communities(G, seed=42)\n",
    "\n",
    "# Compute metrics\n",
    "louvain_results = {\n",
    "    'algorithm': 'Louvain',\n",
    "    'communities': communities_louvain,\n",
    "    'num_communities': len(communities_louvain),\n",
    "    'sizes': [len(c) for c in communities_louvain],\n",
    "    'modularity': compute_modularity(G, communities_louvain),\n",
    "    'coverage': compute_coverage(G, communities_louvain),\n",
    "    'performance': compute_performance(G, communities_louvain),\n",
    "    'diameters': compute_community_diameter(G, communities_louvain),\n",
    "    'densities': compute_community_density(G, communities_louvain),\n",
    "    'clustering': compute_clustering_coefficient(G, communities_louvain),\n",
    "    'conductances': compute_conductance(G, communities_louvain)\n",
    "}\n",
    "\n",
    "print(f\"\\nNumber of communities: {louvain_results['num_communities']}\")\n",
    "print(f\"Modularity: {louvain_results['modularity']:.4f}\")\n",
    "print(f\"Coverage: {louvain_results['coverage']:.4f}\")\n",
    "print(f\"Performance: {louvain_results['performance']:.4f}\")\n",
    "print(f\"Avg community size: {np.mean(louvain_results['sizes']):.2f}\")\n",
    "print(f\"Avg diameter: {np.mean([d for d in louvain_results['diameters'] if d > 0]):.2f}\")\n",
    "print(f\"Avg density: {np.mean(louvain_results['densities']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Label Propagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LABEL PROPAGATION ALGORITHM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run Label Propagation\n",
    "communities_lp = list(community.label_propagation_communities(G))\n",
    "\n",
    "# Compute metrics\n",
    "lp_results = {\n",
    "    'algorithm': 'Label Propagation',\n",
    "    'communities': communities_lp,\n",
    "    'num_communities': len(communities_lp),\n",
    "    'sizes': [len(c) for c in communities_lp],\n",
    "    'modularity': compute_modularity(G, communities_lp),\n",
    "    'coverage': compute_coverage(G, communities_lp),\n",
    "    'performance': compute_performance(G, communities_lp),\n",
    "    'diameters': compute_community_diameter(G, communities_lp),\n",
    "    'densities': compute_community_density(G, communities_lp),\n",
    "    'clustering': compute_clustering_coefficient(G, communities_lp),\n",
    "    'conductances': compute_conductance(G, communities_lp)\n",
    "}\n",
    "\n",
    "print(f\"\\nNumber of communities: {lp_results['num_communities']}\")\n",
    "print(f\"Modularity: {lp_results['modularity']:.4f}\")\n",
    "print(f\"Coverage: {lp_results['coverage']:.4f}\")\n",
    "print(f\"Performance: {lp_results['performance']:.4f}\")\n",
    "print(f\"Avg community size: {np.mean(lp_results['sizes']):.2f}\")\n",
    "print(f\"Avg diameter: {np.mean([d for d in lp_results['diameters'] if d > 0]):.2f}\")\n",
    "print(f\"Avg density: {np.mean(lp_results['densities']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Greedy Modularity Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GREEDY MODULARITY OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run Greedy Modularity\n",
    "communities_greedy = list(community.greedy_modularity_communities(G))\n",
    "\n",
    "# Compute metrics\n",
    "greedy_results = {\n",
    "    'algorithm': 'Greedy Modularity',\n",
    "    'communities': communities_greedy,\n",
    "    'num_communities': len(communities_greedy),\n",
    "    'sizes': [len(c) for c in communities_greedy],\n",
    "    'modularity': compute_modularity(G, communities_greedy),\n",
    "    'coverage': compute_coverage(G, communities_greedy),\n",
    "    'performance': compute_performance(G, communities_greedy),\n",
    "    'diameters': compute_community_diameter(G, communities_greedy),\n",
    "    'densities': compute_community_density(G, communities_greedy),\n",
    "    'clustering': compute_clustering_coefficient(G, communities_greedy),\n",
    "    'conductances': compute_conductance(G, communities_greedy)\n",
    "}\n",
    "\n",
    "print(f\"\\nNumber of communities: {greedy_results['num_communities']}\")\n",
    "print(f\"Modularity: {greedy_results['modularity']:.4f}\")\n",
    "print(f\"Coverage: {greedy_results['coverage']:.4f}\")\n",
    "print(f\"Performance: {greedy_results['performance']:.4f}\")\n",
    "print(f\"Avg community size: {np.mean(greedy_results['sizes']):.2f}\")\n",
    "print(f\"Avg diameter: {np.mean([d for d in greedy_results['diameters'] if d > 0]):.2f}\")\n",
    "print(f\"Avg density: {np.mean(greedy_results['densities']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Girvan-Newman Algorithm (Optional - Slow for large graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run on smaller graphs (< 200 nodes)\n",
    "if G.number_of_nodes() < 200:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GIRVAN-NEWMAN ALGORITHM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Run Girvan-Newman\n",
    "    comp = community.girvan_newman(G)\n",
    "    \n",
    "    # Find optimal communities based on modularity\n",
    "    best_modularity = -1\n",
    "    best_communities = None\n",
    "    \n",
    "    for i, communities_gn in enumerate(comp):\n",
    "        if i > 20:  # Limit iterations\n",
    "            break\n",
    "        mod = compute_modularity(G, communities_gn)\n",
    "        if mod > best_modularity:\n",
    "            best_modularity = mod\n",
    "            best_communities = communities_gn\n",
    "    \n",
    "    communities_gn = list(best_communities)\n",
    "    \n",
    "    # Compute metrics\n",
    "    gn_results = {\n",
    "        'algorithm': 'Girvan-Newman',\n",
    "        'communities': communities_gn,\n",
    "        'num_communities': len(communities_gn),\n",
    "        'sizes': [len(c) for c in communities_gn],\n",
    "        'modularity': compute_modularity(G, communities_gn),\n",
    "        'coverage': compute_coverage(G, communities_gn),\n",
    "        'performance': compute_performance(G, communities_gn),\n",
    "        'diameters': compute_community_diameter(G, communities_gn),\n",
    "        'densities': compute_community_density(G, communities_gn),\n",
    "        'clustering': compute_clustering_coefficient(G, communities_gn),\n",
    "        'conductances': compute_conductance(G, communities_gn)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nNumber of communities: {gn_results['num_communities']}\")\n",
    "    print(f\"Modularity: {gn_results['modularity']:.4f}\")\n",
    "    print(f\"Coverage: {gn_results['coverage']:.4f}\")\n",
    "    print(f\"Performance: {gn_results['performance']:.4f}\")\n",
    "else:\n",
    "    print(\"\\nSkipping Girvan-Newman (graph too large)\")\n",
    "    gn_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = [louvain_results, lp_results, greedy_results]\n",
    "if gn_results:\n",
    "    all_results.append(gn_results)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for r in all_results:\n",
    "    comparison_data.append({\n",
    "        'Algorithm': r['algorithm'],\n",
    "        'Communities': r['num_communities'],\n",
    "        'Modularity': f\"{r['modularity']:.4f}\",\n",
    "        'Coverage': f\"{r['coverage']:.4f}\",\n",
    "        'Performance': f\"{r['performance']:.4f}\",\n",
    "        'Avg Size': f\"{np.mean(r['sizes']):.1f}\",\n",
    "        'Avg Density': f\"{np.mean(r['densities']):.4f}\",\n",
    "        'Avg Clustering': f\"{np.mean(r['clustering']):.4f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best algorithm for each metric\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST ALGORITHM BY METRIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_modularity = max(all_results, key=lambda x: x['modularity'])\n",
    "best_coverage = max(all_results, key=lambda x: x['coverage'])\n",
    "best_performance = max(all_results, key=lambda x: x['performance'])\n",
    "\n",
    "print(f\"Best Modularity: {best_modularity['algorithm']} ({best_modularity['modularity']:.4f})\")\n",
    "print(f\"Best Coverage: {best_coverage['algorithm']} ({best_coverage['coverage']:.4f})\")\n",
    "print(f\"Best Performance: {best_performance['algorithm']} ({best_performance['performance']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Quality Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Community Detection Quality Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "algorithms = [r['algorithm'] for r in all_results]\n",
    "\n",
    "# Modularity\n",
    "modularity = [r['modularity'] for r in all_results]\n",
    "axes[0, 0].bar(algorithms, modularity, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Modularity', fontweight='bold')\n",
    "axes[0, 0].set_title('Modularity (Higher is Better)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Coverage\n",
    "coverage = [r['coverage'] for r in all_results]\n",
    "axes[0, 1].bar(algorithms, coverage, color='coral', alpha=0.7)\n",
    "axes[0, 1].set_ylabel('Coverage', fontweight='bold')\n",
    "axes[0, 1].set_title('Coverage (Higher is Better)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Performance\n",
    "performance = [r['performance'] for r in all_results]\n",
    "axes[1, 0].bar(algorithms, performance, color='mediumseagreen', alpha=0.7)\n",
    "axes[1, 0].set_ylabel('Performance', fontweight='bold')\n",
    "axes[1, 0].set_title('Performance (Higher is Better)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Number of communities\n",
    "num_comms = [r['num_communities'] for r in all_results]\n",
    "axes[1, 1].bar(algorithms, num_comms, color='mediumpurple', alpha=0.7)\n",
    "axes[1, 1].set_ylabel('Number of Communities', fontweight='bold')\n",
    "axes[1, 1].set_title('Number of Communities Detected')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Community Size Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_algos = len(all_results)\n",
    "n_cols = 2\n",
    "n_rows = (n_algos + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5*n_rows))\n",
    "if n_algos == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "fig.suptitle('Community Size Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    sizes = result['sizes']\n",
    "    axes[idx].hist(sizes, bins=min(30, len(set(sizes))), color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel('Community Size', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Frequency', fontweight='bold')\n",
    "    axes[idx].set_title(f\"{result['algorithm']}\\n(Avg: {np.mean(sizes):.1f}, Median: {np.median(sizes):.1f})\")\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_algos, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Density vs Diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5*n_rows))\n",
    "if n_algos == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "fig.suptitle('Community Density vs Diameter', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, result in enumerate(all_results):\n",
    "    densities = result['densities']\n",
    "    diameters = [d if d > 0 else 0 for d in result['diameters']]\n",
    "    axes[idx].scatter(densities, diameters, alpha=0.6, s=50)\n",
    "    axes[idx].set_xlabel('Density', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Diameter', fontweight='bold')\n",
    "    axes[idx].set_title(result['algorithm'])\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_algos, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using best algorithm (Louvain)\n",
    "communities_viz = communities_louvain\n",
    "\n",
    "# Create community color mapping\n",
    "node_colors = {}\n",
    "color_map = plt.cm.tab20.colors + plt.cm.tab20b.colors + plt.cm.tab20c.colors\n",
    "\n",
    "for idx, comm in enumerate(communities_viz):\n",
    "    color = color_map[idx % len(color_map)]\n",
    "    for node in comm:\n",
    "        node_colors[node] = color\n",
    "\n",
    "# Get largest connected component for visualization\n",
    "if not nx.is_connected(G):\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G_viz = G.subgraph(largest_cc).copy()\n",
    "    print(f\"Visualizing largest component with {len(largest_cc)} nodes\")\n",
    "else:\n",
    "    G_viz = G\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Spring layout\n",
    "pos = nx.spring_layout(G_viz, seed=42, k=0.5, iterations=50)\n",
    "node_color_list = [node_colors.get(node, 'gray') for node in G_viz.nodes()]\n",
    "\n",
    "nx.draw_networkx_nodes(G_viz, pos, node_color=node_color_list, node_size=100, alpha=0.8, ax=axes[0])\n",
    "nx.draw_networkx_edges(G_viz, pos, alpha=0.2, width=0.5, ax=axes[0])\n",
    "axes[0].set_title('Spring Layout (Colored by Community)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Circular layout by community\n",
    "communities_viz_filtered = []\n",
    "for comm in communities_viz:\n",
    "    comm_in_viz = [n for n in comm if n in G_viz]\n",
    "    if comm_in_viz:\n",
    "        communities_viz_filtered.append(comm_in_viz)\n",
    "\n",
    "pos_circular = {}\n",
    "angle_step = 2 * np.pi / len(communities_viz_filtered)\n",
    "\n",
    "for idx, comm in enumerate(communities_viz_filtered):\n",
    "    angle = idx * angle_step\n",
    "    radius = 1.0\n",
    "    comm_angle_step = 2 * np.pi / len(comm) if len(comm) > 1 else 0\n",
    "    for node_idx, node in enumerate(comm):\n",
    "        node_angle = angle + node_idx * comm_angle_step * 0.3\n",
    "        x = radius * np.cos(angle) + 0.3 * np.cos(node_angle)\n",
    "        y = radius * np.sin(angle) + 0.3 * np.sin(node_angle)\n",
    "        pos_circular[node] = np.array([x, y])\n",
    "\n",
    "nx.draw_networkx_nodes(G_viz, pos_circular, node_color=node_color_list, node_size=100, alpha=0.8, ax=axes[1])\n",
    "nx.draw_networkx_edges(G_viz, pos_circular, alpha=0.2, width=0.5, ax=axes[1])\n",
    "axes[1].set_title('Circular Layout by Community', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [d for n, d in G.degree()]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(degrees, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Degree', fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[0].set_title('Degree Distribution', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Log-log plot\n",
    "degree_count = Counter(degrees)\n",
    "x = sorted(degree_count.keys())\n",
    "y = [degree_count[k] for k in x]\n",
    "\n",
    "axes[1].loglog(x, y, 'o-', color='coral', markersize=6, alpha=0.7)\n",
    "axes[1].set_xlabel('Degree (log scale)', fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency (log scale)', fontweight='bold')\n",
    "axes[1].set_title('Degree Distribution (Log-Log)', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Relation Types Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 15 relations\n",
    "top_relations = relation_counts.most_common(15)\n",
    "relations, counts = zip(*top_relations)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.barh(range(len(relations)), counts, color='mediumseagreen', alpha=0.7)\n",
    "ax.set_yticks(range(len(relations)))\n",
    "ax.set_yticklabels(relations)\n",
    "ax.set_xlabel('Frequency', fontweight='bold')\n",
    "ax.set_title('Top 15 Relation Types in Knowledge Graph', fontweight='bold', fontsize=14)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary CSV\n",
    "comparison_df.to_csv('community_detection_summary.csv', index=False)\n",
    "print(\"✓ Saved: community_detection_summary.csv\")\n",
    "\n",
    "# Save detailed per-community metrics for each algorithm\n",
    "for result in all_results:\n",
    "    algo_name = result['algorithm'].replace(' ', '_').lower()\n",
    "    comm_df = pd.DataFrame({\n",
    "        'community_id': range(len(result['sizes'])),\n",
    "        'size': result['sizes'],\n",
    "        'diameter': result['diameters'],\n",
    "        'density': result['densities'],\n",
    "        'clustering': result['clustering'],\n",
    "        'conductance': result['conductances']\n",
    "    })\n",
    "    filename = f'communities_{algo_name}.csv'\n",
    "    comm_df.to_csv(filename, index=False)\n",
    "    print(f\"✓ Saved: {filename}\")\n",
    "\n",
    "print(\"\\n✓ All results exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nGraph Statistics:\")\n",
    "print(f\"  Total Nodes: {G.number_of_nodes():,}\")\n",
    "print(f\"  Total Edges: {G.number_of_edges():,}\")\n",
    "print(f\"  Graph Density: {nx.density(G):.6f}\")\n",
    "print(f\"  Average Clustering: {nx.average_clustering(G):.4f}\")\n",
    "\n",
    "print(f\"\\nBest Algorithm Overall: {best_modularity['algorithm']}\")\n",
    "print(f\"  Communities Detected: {best_modularity['num_communities']}\")\n",
    "print(f\"  Modularity: {best_modularity['modularity']:.4f}\")\n",
    "print(f\"  Coverage: {best_modularity['coverage']:.4f}\")\n",
    "print(f\"  Performance: {best_modularity['performance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Key Insights:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_modularity['modularity'] > 0.7:\n",
    "    print(\"✓ Excellent community structure detected (Modularity > 0.7)\")\n",
    "elif best_modularity['modularity'] > 0.3:\n",
    "    print(\"✓ Good community structure detected (Modularity > 0.3)\")\n",
    "else:\n",
    "    print(\"⚠ Weak community structure (Modularity < 0.3)\")\n",
    "\n",
    "if best_coverage['coverage'] > 0.9:\n",
    "    print(\"✓ Very high coverage - most edges within communities\")\n",
    "elif best_coverage['coverage'] > 0.7:\n",
    "    print(\"✓ Good coverage - majority of edges within communities\")\n",
    "\n",
    "avg_conductance = np.mean(best_modularity['conductances'])\n",
    "if avg_conductance < 0.1:\n",
    "    print(\"✓ Low conductance - well-separated communities\")\n",
    "elif avg_conductance < 0.3:\n",
    "    print(\"✓ Moderate conductance - reasonably separated communities\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
