{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Completion using DistMult\n",
    "\n",
    "This notebook implements link prediction on a family relationship knowledge graph using the DistMult embedding method.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load and preprocess the knowledge graph data\n",
    "2. Implement DistMult model\n",
    "3. Train the model with negative sampling\n",
    "4. Evaluate using MRR, Hits@1, and Hits@10\n",
    "5. Visualize training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13821 training triples\n",
      "Loaded 590 test triples\n",
      "\n",
      "Example training triples:\n",
      "  ('olivia0', 'sisterOf', 'selina10')\n",
      "  ('olivia0', 'sisterOf', 'isabella11')\n",
      "  ('olivia0', 'sisterOf', 'oskar24')\n",
      "  ('olivia0', 'sisterOf', 'adam9')\n",
      "  ('olivia0', 'secondAuntOf', 'lena18')\n"
     ]
    }
   ],
   "source": [
    "def load_triples(file_path):\n",
    "    \"\"\"\n",
    "    Load triples from file.\n",
    "    Expected format: head\\trelation\\ttail or head relation tail\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # Try tab-separated first, then space-separated\n",
    "            parts = line.split('\\t') if '\\t' in line else line.split()\n",
    "            if len(parts) == 3:\n",
    "                head, relation, tail = parts\n",
    "                triples.append((head.strip(), relation.strip(), tail.strip()))\n",
    "    return triples\n",
    "\n",
    "# Load train and test data\n",
    "train_triples = load_triples('train.txt')\n",
    "test_triples = load_triples('test.txt')\n",
    "\n",
    "print(f\"Loaded {len(train_triples)} training triples\")\n",
    "print(f\"Loaded {len(test_triples)} test triples\")\n",
    "print(f\"\\nExample training triples:\")\n",
    "for i in range(min(5, len(train_triples))):\n",
    "    print(f\"  {train_triples[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of entities: 1316\n",
      "Number of relations: 28\n",
      "\n",
      "Relation types: ['auntOf', 'boyCousinOf', 'boyFirstCousinOnceRemovedOf', 'boySecondCousinOf', 'brotherOf', 'daughterOf', 'fatherOf', 'girlCousinOf', 'girlFirstCousinOnceRemovedOf', 'girlSecondCousinOf', 'granddaughterOf', 'grandfatherOf', 'grandmotherOf', 'grandsonOf', 'greatAuntOf', 'greatGranddaughterOf', 'greatGrandfatherOf', 'greatGrandmotherOf', 'greatGrandsonOf', 'greatUncleOf', 'motherOf', 'nephewOf', 'nieceOf', 'secondAuntOf', 'secondUncleOf', 'sisterOf', 'sonOf', 'uncleOf']\n"
     ]
    }
   ],
   "source": [
    "def create_mappings(train_triples, test_triples):\n",
    "    \"\"\"\n",
    "    Create mappings from entities/relations to IDs.\n",
    "    \"\"\"\n",
    "    entities = set()\n",
    "    relations = set()\n",
    "    \n",
    "    # Collect all entities and relations\n",
    "    for h, r, t in train_triples + test_triples:\n",
    "        entities.add(h)\n",
    "        entities.add(t)\n",
    "        relations.add(r)\n",
    "    \n",
    "    # Create ID mappings\n",
    "    entity2id = {entity: idx for idx, entity in enumerate(sorted(entities))}\n",
    "    relation2id = {relation: idx for idx, relation in enumerate(sorted(relations))}\n",
    "    \n",
    "    id2entity = {idx: entity for entity, idx in entity2id.items()}\n",
    "    id2relation = {idx: relation for relation, idx in relation2id.items()}\n",
    "    \n",
    "    return entity2id, relation2id, id2entity, id2relation\n",
    "\n",
    "entity2id, relation2id, id2entity, id2relation = create_mappings(train_triples, test_triples)\n",
    "\n",
    "print(f\"\\nNumber of entities: {len(entity2id)}\")\n",
    "print(f\"Number of relations: {len(relation2id)}\")\n",
    "print(f\"\\nRelation types: {list(relation2id.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (13821, 3)\n",
      "Test data shape: (590, 3)\n"
     ]
    }
   ],
   "source": [
    "def triples_to_ids(triples, entity2id, relation2id):\n",
    "    \"\"\"\n",
    "    Convert string triples to ID triples.\n",
    "    \"\"\"\n",
    "    id_triples = []\n",
    "    for h, r, t in triples:\n",
    "        id_triples.append((\n",
    "            entity2id[h],\n",
    "            relation2id[r],\n",
    "            entity2id[t]\n",
    "        ))\n",
    "    return np.array(id_triples)\n",
    "\n",
    "train_data = triples_to_ids(train_triples, entity2id, relation2id)\n",
    "test_data = triples_to_ids(test_triples, entity2id, relation2id)\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 13821 samples\n",
      "Batch size: 128, Number of batches: 108\n"
     ]
    }
   ],
   "source": [
    "class KGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Knowledge Graph Dataset with negative sampling.\n",
    "    \"\"\"\n",
    "    def __init__(self, triples, num_entities, num_neg_samples=1):\n",
    "        self.triples = triples\n",
    "        self.num_entities = num_entities\n",
    "        self.num_neg_samples = num_neg_samples\n",
    "        \n",
    "        # Create set of all true triples for filtering\n",
    "        self.true_triples = set([tuple(triple) for triple in triples])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        positive_triple = self.triples[idx]\n",
    "        head, relation, tail = positive_triple\n",
    "        \n",
    "        # Generate negative samples by corrupting tail only\n",
    "        neg_tails = []\n",
    "        for _ in range(self.num_neg_samples):\n",
    "            neg_tail = random.randint(0, self.num_entities - 1)\n",
    "            neg_triple = (head, relation, neg_tail)\n",
    "            \n",
    "            # Ensure it's not a true triple\n",
    "            attempts = 0\n",
    "            while neg_triple in self.true_triples and attempts < 10:\n",
    "                neg_tail = random.randint(0, self.num_entities - 1)\n",
    "                neg_triple = (head, relation, neg_tail)\n",
    "                attempts += 1\n",
    "            \n",
    "            neg_tails.append(neg_tail)\n",
    "        \n",
    "        return (\n",
    "            torch.LongTensor([head]),\n",
    "            torch.LongTensor([relation]),\n",
    "            torch.LongTensor([tail]),\n",
    "            torch.LongTensor(neg_tails)\n",
    "        )\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = KGDataset(train_data, len(entity2id), num_neg_samples=10)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"Created dataset with {len(train_dataset)} samples\")\n",
    "print(f\"Batch size: 128, Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement DistMult Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 134400 parameters\n",
      "DistMult(\n",
      "  (entity_embeddings): Embedding(1316, 100)\n",
      "  (relation_embeddings): Embedding(28, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DistMult(nn.Module):\n",
    "    \"\"\"\n",
    "    DistMult: Embedding Entities with Bilinear Model\n",
    "    \n",
    "    Score function: score(h, r, t) = <h, r, t> = sum(h * r * t)\n",
    "    where h, r, t are embedding vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim=100):\n",
    "        super(DistMult, self).__init__()\n",
    "        \n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Entity embeddings\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        \n",
    "        # Relation embeddings\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.xavier_uniform_(self.entity_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_embeddings.weight)\n",
    "    \n",
    "    def forward(self, heads, relations, tails):\n",
    "        \"\"\"\n",
    "        Compute scores for given triples.\n",
    "        \n",
    "        Args:\n",
    "            heads: (batch_size,) or (batch_size, num_samples)\n",
    "            relations: (batch_size,) or (batch_size, 1)\n",
    "            tails: (batch_size,) or (batch_size, num_samples)\n",
    "        \n",
    "        Returns:\n",
    "            scores: (batch_size,) or (batch_size, num_samples)\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        h_emb = self.entity_embeddings(heads)  # (batch, dim) or (batch, num_samples, dim)\n",
    "        r_emb = self.relation_embeddings(relations)  # (batch, dim)\n",
    "        t_emb = self.entity_embeddings(tails)  # (batch, dim) or (batch, num_samples, dim)\n",
    "        \n",
    "        # Expand relation embeddings if needed\n",
    "        if len(h_emb.shape) == 3:  # (batch, num_samples, dim)\n",
    "            r_emb = r_emb.unsqueeze(1)  # (batch, 1, dim)\n",
    "        \n",
    "        # DistMult score: element-wise multiplication then sum\n",
    "        scores = torch.sum(h_emb * r_emb * t_emb, dim=-1)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def score_all_tails(self, heads, relations):\n",
    "        \"\"\"\n",
    "        Score all possible tails for given (head, relation) pairs.\n",
    "        Used for evaluation.\n",
    "        \n",
    "        Args:\n",
    "            heads: (batch_size,)\n",
    "            relations: (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            scores: (batch_size, num_entities)\n",
    "        \"\"\"\n",
    "        h_emb = self.entity_embeddings(heads)  # (batch, dim)\n",
    "        r_emb = self.relation_embeddings(relations)  # (batch, dim)\n",
    "        all_t_emb = self.entity_embeddings.weight  # (num_entities, dim)\n",
    "        \n",
    "        # Compute scores for all tails\n",
    "        hr = h_emb * r_emb\n",
    "        scores = torch.mm(hr, all_t_emb.t())\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def score_all_heads(self, relations, tails):\n",
    "        \"\"\"\n",
    "        Score all possible heads for given (relation, tail) pairs.\n",
    "        Used for evaluation.\n",
    "        \n",
    "        Args:\n",
    "            relations: (batch_size,)\n",
    "            tails: (batch_size,)\n",
    "        \n",
    "        Returns:\n",
    "            scores: (batch_size, num_entities)\n",
    "        \"\"\"\n",
    "        r_emb = self.relation_embeddings(relations)  # (batch, dim)\n",
    "        t_emb = self.entity_embeddings(tails)  # (batch, dim)\n",
    "        all_h_emb = self.entity_embeddings.weight  # (num_entities, dim)\n",
    "        \n",
    "        # Compute scores for all heads\n",
    "        rt = r_emb * t_emb\n",
    "        scores = torch.mm(rt, all_h_emb.t())\n",
    "        \n",
    "        return scores\n",
    "\n",
    "# Initialize model\n",
    "model = DistMult(\n",
    "    num_entities=len(entity2id),\n",
    "    num_relations=len(relation2id),\n",
    "    embedding_dim=100\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using margin ranking loss with margin=1.0\n",
      "Optimizer: Adam with learning rate 0.001\n"
     ]
    }
   ],
   "source": [
    "def margin_loss(positive_scores, negative_scores, margin=1.0):\n",
    "    \"\"\"\n",
    "    Margin-based ranking loss.\n",
    "    L = max(0, margin - positive_score + negative_score)\n",
    "    \"\"\"\n",
    "    # Ensure dimensions match\n",
    "    # positive_scores: (batch_size,)\n",
    "    # negative_scores: (batch_size, num_neg_samples)\n",
    "    \n",
    "    if len(negative_scores.shape) == 2:\n",
    "        positive_scores = positive_scores.unsqueeze(1)  # (batch_size, 1)\n",
    "    \n",
    "    loss = torch.relu(margin - positive_scores + negative_scores)\n",
    "    return loss.mean()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Using margin ranking loss with margin=1.0\")\n",
    "print(f\"Optimizer: Adam with learning rate 0.001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 100 epochs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (10) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     train_losses.append(loss)\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, device)\u001b[39m\n\u001b[32m     30\u001b[39m negative_scores = model(heads_expanded, relations_expanded, neg_tails)  \u001b[38;5;66;03m# (batch_size, num_neg)\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m loss = \u001b[43mmargin_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     36\u001b[39m loss.backward()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mmargin_loss\u001b[39m\u001b[34m(positive_scores, negative_scores, margin)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(negative_scores.shape) == \u001b[32m2\u001b[39m:\n\u001b[32m     11\u001b[39m     positive_scores = positive_scores.unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m loss = torch.relu(\u001b[43mmargin\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive_scores\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_scores\u001b[49m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.mean()\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (128) must match the size of tensor b (10) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        heads, relations, tails, neg_tails = batch\n",
    "        \n",
    "        # Move to device and squeeze\n",
    "        heads = heads.squeeze(1).to(device)  # (batch_size,)\n",
    "        relations = relations.squeeze(1).to(device)  # (batch_size,)\n",
    "        tails = tails.squeeze(1).to(device)  # (batch_size,)\n",
    "        neg_tails = neg_tails.to(device)  # (batch_size, num_neg_samples)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute positive scores\n",
    "        positive_scores = model(heads, relations, tails)  # (batch_size,)\n",
    "        \n",
    "        # Expand heads and relations for negative samples\n",
    "        batch_size = heads.size(0)\n",
    "        num_neg = neg_tails.size(1)\n",
    "        \n",
    "        heads_expanded = heads.unsqueeze(1).expand(batch_size, num_neg)  # (batch_size, num_neg)\n",
    "        relations_expanded = relations.unsqueeze(1).expand(batch_size, num_neg)  # (batch_size, num_neg)\n",
    "        \n",
    "        # Compute negative scores\n",
    "        negative_scores = model(heads_expanded, relations_expanded, neg_tails)  # (batch_size, num_neg)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = margin_loss(positive_scores, negative_scores, margin=1.0)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "print(f\"\\nStarting training for {num_epochs} epochs...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('DistMult Training Loss over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Plot saved as 'training_loss.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_rank(model, test_triple, all_triples, num_entities, device, mode='tail'):\n",
    "    \"\"\"\n",
    "    Get filtered rank for a test triple.\n",
    "    \n",
    "    Args:\n",
    "        test_triple: (head, relation, tail) as IDs\n",
    "        all_triples: set of all known triples (train + test)\n",
    "        mode: 'tail' or 'head' prediction\n",
    "    \n",
    "    Returns:\n",
    "        rank: filtered rank of the true entity\n",
    "    \"\"\"\n",
    "    head, relation, tail = test_triple\n",
    "    \n",
    "    if mode == 'tail':\n",
    "        # Predict tail given (head, relation)\n",
    "        heads_tensor = torch.LongTensor([head]).to(device)\n",
    "        relations_tensor = torch.LongTensor([relation]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = model.score_all_tails(heads_tensor, relations_tensor).squeeze()\n",
    "        \n",
    "        # Filter out other true triples\n",
    "        for i in range(num_entities):\n",
    "            if i != tail and (head, relation, i) in all_triples:\n",
    "                scores[i] = -1e10  # Set to very low score\n",
    "        \n",
    "        # Get rank of true tail\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        rank = (sorted_indices == tail).nonzero(as_tuple=True)[0].item() + 1\n",
    "        \n",
    "    else:  # mode == 'head'\n",
    "        # Predict head given (relation, tail)\n",
    "        relations_tensor = torch.LongTensor([relation]).to(device)\n",
    "        tails_tensor = torch.LongTensor([tail]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = model.score_all_heads(relations_tensor, tails_tensor).squeeze()\n",
    "        \n",
    "        # Filter out other true triples\n",
    "        for i in range(num_entities):\n",
    "            if i != head and (i, relation, tail) in all_triples:\n",
    "                scores[i] = -1e10\n",
    "        \n",
    "        # Get rank of true head\n",
    "        sorted_indices = torch.argsort(scores, descending=True)\n",
    "        rank = (sorted_indices == head).nonzero(as_tuple=True)[0].item() + 1\n",
    "    \n",
    "    return rank\n",
    "\n",
    "def evaluate(model, test_data, all_triples, num_entities, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test data.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with MRR, Hits@1, Hits@3, Hits@10\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    ranks_tail = []\n",
    "    ranks_head = []\n",
    "    \n",
    "    print(\"Evaluating on test set...\")\n",
    "    \n",
    "    for triple in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        # Tail prediction\n",
    "        rank_tail = get_filtered_rank(model, triple, all_triples, num_entities, device, mode='tail')\n",
    "        ranks_tail.append(rank_tail)\n",
    "        \n",
    "        # Head prediction\n",
    "        rank_head = get_filtered_rank(model, triple, all_triples, num_entities, device, mode='head')\n",
    "        ranks_head.append(rank_head)\n",
    "    \n",
    "    # Combine ranks from both directions\n",
    "    all_ranks = ranks_tail + ranks_head\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mrr = np.mean([1.0 / rank for rank in all_ranks])\n",
    "    hits_at_1 = np.mean([1.0 if rank <= 1 else 0.0 for rank in all_ranks])\n",
    "    hits_at_3 = np.mean([1.0 if rank <= 3 else 0.0 for rank in all_ranks])\n",
    "    hits_at_10 = np.mean([1.0 if rank <= 10 else 0.0 for rank in all_ranks])\n",
    "    \n",
    "    results = {\n",
    "        'MRR': mrr,\n",
    "        'Hits@1': hits_at_1,\n",
    "        'Hits@3': hits_at_3,\n",
    "        'Hits@10': hits_at_10,\n",
    "        'ranks_tail': ranks_tail,\n",
    "        'ranks_head': ranks_head\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set of all triples (train + test) for filtering\n",
    "all_triples = set([tuple(triple) for triple in train_data] + [tuple(triple) for triple in test_data])\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate(model, test_data, all_triples, len(entity2id), device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "print(f\"MRR:      {results['MRR']:.4f}\")\n",
    "print(f\"Hits@1:   {results['Hits@1']:.4f}\")\n",
    "print(f\"Hits@3:   {results['Hits@3']:.4f}\")\n",
    "print(f\"Hits@10:  {results['Hits@10']:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of metrics\n",
    "metrics = ['MRR', 'Hits@1', 'Hits@3', 'Hits@10']\n",
    "values = [results['MRR'], results['Hits@1'], results['Hits@3'], results['Hits@10']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics, values, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'], alpha=0.8)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('DistMult Evaluation Metrics on Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{value:.4f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved as 'evaluation_metrics.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyze Rank Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rank distribution\n",
    "all_ranks = results['ranks_tail'] + results['ranks_head']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(all_ranks, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Rank', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Ranks', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# CDF\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_ranks = np.sort(all_ranks)\n",
    "cdf = np.arange(1, len(sorted_ranks) + 1) / len(sorted_ranks)\n",
    "plt.plot(sorted_ranks, cdf, linewidth=2, color='#e74c3c')\n",
    "plt.xlabel('Rank', fontsize=12)\n",
    "plt.ylabel('Cumulative Probability', fontsize=12)\n",
    "plt.title('Cumulative Distribution of Ranks', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rank_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean rank: {np.mean(all_ranks):.2f}\")\n",
    "print(f\"Median rank: {np.median(all_ranks):.0f}\")\n",
    "print(f\"Plot saved as 'rank_distribution.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tail(model, head_name, relation_name, entity2id, relation2id, id2entity, top_k=5):\n",
    "    \"\"\"\n",
    "    Predict top-k tails for a given (head, relation) pair.\n",
    "    \"\"\"\n",
    "    head_id = entity2id[head_name]\n",
    "    relation_id = relation2id[relation_name]\n",
    "    \n",
    "    heads_tensor = torch.LongTensor([head_id]).to(device)\n",
    "    relations_tensor = torch.LongTensor([relation_id]).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = model.score_all_tails(heads_tensor, relations_tensor).squeeze()\n",
    "    \n",
    "    # Get top-k predictions\n",
    "    top_scores, top_indices = torch.topk(scores, k=top_k)\n",
    "    \n",
    "    predictions = []\n",
    "    for score, idx in zip(top_scores, top_indices):\n",
    "        tail_name = id2entity[idx.item()]\n",
    "        predictions.append((tail_name, score.item()))\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get a few test triples for demonstration\n",
    "num_examples = min(3, len(test_triples))\n",
    "for i in range(num_examples):\n",
    "    h, r, t = test_triples[i]\n",
    "    print(f\"\\nQuery: ({h}, {r}, ?)\")\n",
    "    print(f\"True answer: {t}\")\n",
    "    print(f\"\\nTop-5 predictions:\")\n",
    "    \n",
    "    predictions = predict_tail(model, h, r, entity2id, relation2id, id2entity, top_k=5)\n",
    "    for rank, (pred_tail, score) in enumerate(predictions, 1):\n",
    "        marker = \"âœ“\" if pred_tail == t else \" \"\n",
    "        print(f\"  {rank}. {pred_tail:20s} (score: {score:7.4f}) {marker}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'entity2id': entity2id,\n",
    "    'relation2id': relation2id,\n",
    "    'id2entity': id2entity,\n",
    "    'id2relation': id2relation,\n",
    "    'embedding_dim': model.embedding_dim,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'distmult_model.pt')\n",
    "print(\"Model saved as 'distmult_model.pt'\")\n",
    "\n",
    "# To load later:\n",
    "# checkpoint = torch.load('distmult_model.pt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  - Training triples:     {len(train_triples):,}\")\n",
    "print(f\"  - Test triples:         {len(test_triples):,}\")\n",
    "print(f\"  - Number of entities:   {len(entity2id):,}\")\n",
    "print(f\"  - Number of relations:  {len(relation2id):,}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  - Model:                DistMult\")\n",
    "print(f\"  - Embedding dimension:  {model.embedding_dim}\")\n",
    "print(f\"  - Total parameters:     {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  - Training epochs:      {num_epochs}\")\n",
    "print(f\"  - Final training loss:  {train_losses[-1]:.4f}\")\n",
    "print(f\"\\nTest Set Performance (Filtered):\")\n",
    "print(f\"  - MRR:                  {results['MRR']:.4f}\")\n",
    "print(f\"  - Hits@1:               {results['Hits@1']:.4f}\")\n",
    "print(f\"  - Hits@3:               {results['Hits@3']:.4f}\")\n",
    "print(f\"  - Hits@10:              {results['Hits@10']:.4f}\")\n",
    "print(f\"  - Mean rank:            {np.mean(all_ranks):.2f}\")\n",
    "print(f\"  - Median rank:          {np.median(all_ranks):.0f}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - training_loss.png\")\n",
    "print(\"  - evaluation_metrics.png\")\n",
    "print(\"  - rank_distribution.png\")\n",
    "print(\"  - distmult_model.pt\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
